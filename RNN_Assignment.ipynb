{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AverYuchen/DeepLearning/blob/main/RNN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 2: RNNs for Sentiment Analysis"
      ],
      "metadata": {
        "id": "z5QuE1cviz2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The IMDB dataset consists of 25K positive and 25K negative movie reviews for training, and a further set of 25K positive and 25K negative reviews for testing (a total of 100K reviews), all sourced from [IMDB](https://www.imdb.com/). The training and testing sets are, of course, disjoint.\n",
        "\n",
        "It was produced by NLP researchers at Stanford, the goal being to train intelligent text processing systems to classify the reviews as positive or negative (sometimes referred to as \"[sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)\"), one of the earliest and best-studied NLP tasks.\n",
        "\n",
        "The dataset and its metadata can be accessed via HuggingFace Datasets [here](https://huggingface.co/datasets/stanfordnlp/imdb)."
      ],
      "metadata": {
        "id": "iZZwhnu9vYb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "oPPzLox4OUU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"stanfordnlp/imdb\")"
      ],
      "metadata": {
        "id": "FomgDZAXYwVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a wrapper class that subclasses [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). The point of doing this is to later further wrap our IMDBDataset inside of a `DataLoader`, which has useful functionalities for quickly loading batches of data."
      ],
      "metadata": {
        "id": "9DhMGjx4w2Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "\n",
        "    # any subclass of Dataset must have __init__(self, ...)\n",
        "    def __init__(self, split='train'):\n",
        "\n",
        "        # inherit all functionality from Dataset\n",
        "        super().__init__()\n",
        "\n",
        "        # check that there is one label for every review\n",
        "        assert len(imdb[split]['text']) == len(imdb[split]['label'])\n",
        "\n",
        "        # data structure of our choosing\n",
        "        self.pairs = list(zip(imdb[split]['text'], imdb[split]['label']))\n",
        "\n",
        "    # any subclass of Dataset must have __len__(self)\n",
        "    # x.__len__() is the same as len(x)\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    # any subclass of Dataset must have __getitem__(self, idx)\n",
        "    # x.__getitem__(i) is the same as x[i]\n",
        "    # be careful when writing this method:\n",
        "    # be sure it accommodates slices! x[i:j]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ],
      "metadata": {
        "id": "JPOrzQjjX1NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-nUfymIJY6UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports for RNN Architecture"
      ],
      "metadata": {
        "id": "Td1D41T4mqJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Neural network building blocks: [`torch.nn`](https://pytorch.org/docs/stable/nn.html)\n",
        "- NN functions: [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html)\n",
        "- RNN - combine a batch of padded sequences into a single packed sequence for efficient processing: [`torch.nn.utils.rnn.pack_padded_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\n",
        "- RNN - un-combine packed sequences back into a padded batch: [`torch.nn.utils.rnn.pad_packed_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html)"
      ],
      "metadata": {
        "id": "ClUWjdh82jB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils"
      ],
      "metadata": {
        "id": "zsS9kFYuOqdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN architecture"
      ],
      "metadata": {
        "id": "n5RH5NdRmtBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the CNNs in the last assignment, we must:\n",
        "- subclass `nn.Module`\n",
        "- define an `__init__(self, ...)` method\n",
        "- define a `forward(self, ...)` method"
      ],
      "metadata": {
        "id": "fs_3ftOacNyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A brief note on why we need padding and truncation\n",
        "\n",
        "Although RNNs can handle arbitrarily long sequences, as well as sequences variable in size, we cannot process a batch of sequences of different lengths all at once.\n",
        "\n",
        "Consider a batch like:\n",
        "\n",
        "$$ [0, 4, 8, 9, 27] \\\\ [0, 2] \\\\ [0, 3, 5] $$\n",
        "\n",
        "Let's say we have processed the first two tokens in each sequence using linear algebra operations on the hardware. We still have to process:\n",
        "\n",
        "$$ [8, 9, 27] \\\\ [] \\\\ [5] $$\n",
        "\n",
        "A generic algorithm for handling matrices and vectors is going to throw an error when it sees that there are empty entries here.\n",
        "\n",
        "On the other hand, truncation allows us to specify  a maximum length to pad sequences to, while also handling cases where a sequence is longer than that maximum length. We simply \"cut off\" the sequence beyond that maximum length (this sequence won't actually need any padding)."
      ],
      "metadata": {
        "id": "KxnffpdolvvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TODO: Complete the `__init__()` and `forward()` methods of the following RNN class [10 points].***"
      ],
      "metadata": {
        "id": "KBZB58qGv2wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            dropout,\n",
        "            bidirectional,\n",
        "            padding_idx\n",
        "        ):\n",
        "\n",
        "        # inherit from nn.Module\n",
        "        super().__init__()\n",
        "\n",
        "        # we need these for the forward() method\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        \"\"\"EMBEDDING LAYER\"\"\"\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=, # TODO: need one embedding for each vocab item\n",
        "            embedding_dim=embedding_dim, # customizable\n",
        "            padding_idx=padding_idx # makes sure we treat padding as padding\n",
        "        )\n",
        "\n",
        "        \"\"\"RNN LAYERS (LSTM)\"\"\"\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=, # TODO: input comes from the embedding layer\n",
        "            hidden_size=hidden_size, # customizable\n",
        "            num_layers=num_layers, # customizable\n",
        "            batch_first=True, # generally the right way to order the dims\n",
        "            dropout=dropout, # probability of dropping a connection in the LSTM\n",
        "            bidirectional=bidirectional # choice, may help, may hurt\n",
        "        )\n",
        "\n",
        "        if not bidirectional: # concat last hidden state w/ last final state\n",
        "            self.linear = nn.Linear(2 * hidden_size, 1)\n",
        "        else: # concat first and last hidden states w/ first and last final states\n",
        "            self.linear = nn.Linear(4 * hidden_size, 1)\n",
        "\n",
        "        # quick way to count trainable params in the model\n",
        "        p_count = 0\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad:\n",
        "                p_count += param.numel()\n",
        "        print(f'Model initialized with {p_count} trainable parameters.')\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        #print('input', x.shape) # [batch_size, seq_length]\n",
        "\n",
        "        \"\"\"TODO: EMBED THE SEQUENCE\"\"\"\n",
        "\n",
        "        #print('embed', x.shape) # [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        \"\"\"PACK THE SEQUENCE\"\"\"\n",
        "        # see documentation to understand what this does\n",
        "        x = rnn_utils.pack_padded_sequence(\n",
        "            x,\n",
        "            lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        # can't print shape here, because x is not a torch.Tensor anymore :/\n",
        "\n",
        "        \"\"\"TODO: PASS THE SEQUENCE THROUGH THE RNN\"\"\"\n",
        "\n",
        "        #print('x', x.shape) # [batch_size, seq_length, hidden_size(*2)]\n",
        "        #print('h', hidden.shape) # [num_layers(*2), batch_size, hidden_size]\n",
        "\n",
        "        \"\"\"UNPACK THE SEQUENCE\"\"\"\n",
        "        # this undoes the packing from a few lines ago\n",
        "        x, _ = rnn_utils.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        if self.bidirectional:\n",
        "\n",
        "            \"\"\"GET THE LAST OUTPUT LEFT-TO-RIGHT, IGNORING PADDING\"\"\"\n",
        "            # range(len(x)) = entire batch, lengths - 1 = end of sequences\n",
        "            # ignoring padding, :self.hidden_size = LTR\n",
        "            forward_last_output = x[range(len(x)), lengths - 1, :self.hidden_size]\n",
        "            #print('fwo', forward_last_output.shape) # [batch_size, hidden_size]\n",
        "\n",
        "            \"\"\"GET THE LAST OUTPUT RIGHT-TO-LEFT, IGNORING PADDING\"\"\"\n",
        "            # : = entire batch, 0 = start of sequence, self.hidden_size: = RTL\n",
        "            backward_last_output = x[:, 0, self.hidden_size:]\n",
        "            #print('bwo', backward_last_output.shape) # [batch_size, hidden_size]\n",
        "\n",
        "            \"\"\"GET THE LAST HIDDEN STATE LEFT-TO-RIGHT\"\"\"\n",
        "            # second-to-last item in dim 0 will be the hidden state from\n",
        "            # the full LTR pass\n",
        "            forward_last_hidden = hidden[-2, :, :]\n",
        "            #print('fwh', forward_last_hidden.shape) # [batch_size, hidden_size]\n",
        "\n",
        "            \"\"\"GET THE LAST HIDDEN STATE RIGHT-TO-LEFT\"\"\"\n",
        "            # last item in dim 0 will be the hidden state from the full RTL pass\n",
        "            backward_last_hidden = hidden[-1, :, :]\n",
        "            #print('bwh', backward_last_hidden.shape) # [batch_size, hidden_size]\n",
        "\n",
        "            \"\"\"CONCATENATE THE LAST OUTPUTS AND HIDDEN STATES\"\"\"\n",
        "            # concat allows us to use information from all four sources\n",
        "            x = torch.cat((\n",
        "                forward_last_output,\n",
        "                backward_last_output,\n",
        "                forward_last_hidden,\n",
        "                backward_last_hidden\n",
        "            ), dim=1)\n",
        "            #print('cat', x.shape) # [batch_size, hidden_size*4]\n",
        "\n",
        "        else:\n",
        "\n",
        "            \"\"\"GET THE LAST OUTPUT, IGNORING PADDING\"\"\"\n",
        "            # range(len(x)) = entire batch, lengths - 1 = end of sequences\n",
        "            # ignoring padding, :self.hidden_size = LTR\n",
        "            last_output = x[range(len(x)), lengths - 1, :self.hidden_size]\n",
        "            #print('uni out', last_output.shape) # [batch_size, hidden_size]\n",
        "\n",
        "            \"\"\"GET THE LAST HIDDEN STATE\"\"\"\n",
        "            # last item in dim 0 will be the hidden state from the full LTR pass\n",
        "            last_hidden = hidden[-1, :, :]\n",
        "            #print('uni hidden', last_hidden.shape) # [batch_size, hidden_size]\n",
        "\n",
        "            \"\"\"CONCATENATE THE LAST OUTPUT AND HIDDEN STATE\"\"\"\n",
        "            # concat allows us to use information from both sources\n",
        "            x = torch.cat((\n",
        "                last_output,\n",
        "                last_hidden\n",
        "            ), dim=1)\n",
        "            #print('uni cat', x.shape) # [batch_size, hidden_size*2]\n",
        "\n",
        "        \"\"\"PASS THE OUTPUT THROUGH THE LINEAR LAYER\"\"\"\n",
        "        # .squeeze() removes the extra dimension\n",
        "        x = self.linear(x).squeeze()\n",
        "        #print('sys out', x.shape) # [batch_size]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "y5sUpmqcPoYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "7YPbuogAY_tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a tokenizer yourself is not too painful, but an even easier choice is to load up a pretrained one such as the one used by [BERT](https://bibbase.org/service/mendeley/bfbbf840-4c42-3914-a463-19024f50b30c/file/6375d223-e085-74b3-392f-f3fed829cd72/Devlin_et_al___2019___BERT_Pre_training_of_Deep_Bidirectional_Transform.pdf.pdf). If you're curious about training your own, look into [SentencePiece](https://github.com/google/sentencepiece).\n",
        "\n",
        "Otherwise, let's use BERT's tokenizer, available on [HuggingFace](https://huggingface.co/google-bert/bert-base-uncased):"
      ],
      "metadata": {
        "id": "yT3POfQ_OkmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "# take a look at how it splits up words\n",
        "print('calling tokenizer.encode:')\n",
        "print(tokenizer.encode('hello my name is slim shady'))\n",
        "\n",
        "# tokenizer use, can also just say tokenizer(...) rather than tokenizer.__call__(...)\n",
        "tokens = tokenizer.__call__(\n",
        "    'hello my name is slim shady', # text to tokenize\n",
        "    padding='max_length', # pad sequences to the max_length if necessary\n",
        "    truncation=True, # truncate long sequences if necessary\n",
        "    max_length=128, # truncate and/or pad to this length\n",
        "    return_tensors='pt' # give torch.Tensor as the output of tokenizing\n",
        ")['input_ids'] # there's also two other things in the encoding, we just want the ids\n",
        "\n",
        "# take a look at the input ids\n",
        "print('calling the tokenizer with max length padding:')\n",
        "print(tokens)\n",
        "\n",
        "# how many tokens in the model's vocabulary?\n",
        "print('tokenizer\\'s vocab size:', tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "Rt6E1oFYSekm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation"
      ],
      "metadata": {
        "id": "g60JcMXJmyjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports for training and evaluation"
      ],
      "metadata": {
        "id": "ttB8r4VXmz1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n",
        "- [`torch.nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
        "- [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
        "- [`time.time`](https://docs.python.org/3/library/time.html#time.time)\n",
        "- [`sklearn.metrics.classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
      ],
      "metadata": {
        "id": "ZTXJVQk6s_J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam # optimization algorithm\n",
        "from torch.nn import BCEWithLogitsLoss # loss function\n",
        "from torch.utils.data import DataLoader # iterates over dataset in batches\n",
        "from time import time # time things\n",
        "from sklearn.metrics import classification_report # evaluation metric"
      ],
      "metadata": {
        "id": "l-2vKIXHWQnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "LeOn3SZFZEc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters are things chosen by the programmer, as opposed to parameters, which are optimized by a learning algorithm.\n",
        "\n",
        "Some hyperparameters we are setting:\n",
        "- Vocabulary size, which we will just set to be the same as BERT's for convenience\n",
        "- Embedding size, a choice. It's harder to train/slower/prone to overfitting a larger embedding, but a larger embedding may perform better.\n",
        "- Hidden size, also a choice with the same issues. Usually smaller than the embedding size.\n",
        "- Number of layers, also a choice with the same issues.\n",
        "- Dropout - a little dropout often helps regularize a model and prevents overfitting.\n",
        "- Bidirectional vs. unidirectional - another trade-off. Bidirectional will be harder to train/slower/prone to overfitting, but may perform better.\n",
        "- Padding index - which input id is the padding token which should be ignored.\n",
        "- Batch size - how many inputs to process in parallel, and also, how many inputs to process before updating the model's parameters. A smaller batch size may cause overfitting (over-reliance on just a few examples to update the model), but uses less memory.\n",
        "- Epochs - how many times you want the model to see the training set. More epochs will train the model better, but can cause overfitting.\n",
        "- Maximum length - the longest sequence length we will process. A longer sequence length takes longer to process, but captures more of the information in the sequence. For an RNN, longer sequences can also kill performance, because RNNs may \"forget\" information as they move along the sequence.\n",
        "- Optimization algorithm - Adam is a popular choice, but other choices may do better. Other common choices are [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) and [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW).\n",
        "- Learning rate - THE MOST IMPORTANT HYPERPARAMETER - this has been [shown empirically](https://proceedings.mlr.press/v28/bergstra13.html). Typical choices are orders of magnitude smaller than 1 (1e-2, 1e-3, 1e-4...).\n",
        "- Loss function - kind of a hyperparameter, but really, it determines what you are teaching the model. Binary cross-entropy loss is appropriate for binary classification. BCE will force outputs for the positive class to be larger (greater than 0), and outputs for the negative class to be smaller (less than 0)."
      ],
      "metadata": {
        "id": "tFcVGppQRbKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TODO: Adjust the learning rate and number of epochs to train the model to at least 75% accuracy [10 points]. NOTE: You should do this part last.***"
      ],
      "metadata": {
        "id": "vtAV7OeDy6zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available, and let device be 'cuda' if so, otherwise 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# our model\n",
        "rnn = RNN(\n",
        "    vocab_size=tokenizer.vocab_size, # how many embeddings we need\n",
        "    embedding_dim=256, # number of dimensions in an embedding vector\n",
        "    hidden_size=128, # number of dimensions in each hidden layer\n",
        "    num_layers=4, # number of RNN layers\n",
        "    dropout=0.1, # dropout probability in each RNN layer\n",
        "    bidirectional=True, # if False, only read left-to-right\n",
        "    padding_idx=tokenizer.pad_token_id # ignore padding tokens in the input\n",
        ").to(device) # move model onto GPU if available\n",
        "\n",
        "# number of examples to process in parallel\n",
        "# we will perform only one optimization step per batch\n",
        "batch_size = 16\n",
        "\n",
        "# number of times the model sees the training data\n",
        "epochs = 1 # TODO: select a better number of epochs\n",
        "\n",
        "# maximum length of sequences we will process\n",
        "# longer sequences will be truncated to this length\n",
        "# shorter sequences will be padded to this length\n",
        "max_length = 256\n",
        "\n",
        "# Adam is the most beloved optimizer, 'lr' is the learning rate\n",
        "optimizer = Adam(rnn.parameters(), lr=1) # TODO: select a better learning rate\n",
        "\n",
        "# loss function for binary classification\n",
        "criterion = BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "5IWObskLaf9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing a forward pass to make sure shapes are compatible\n",
        "\n",
        "We are picking a random tensor for both `x` and `lengths` to make sure that the model functions correctly.\n",
        "\n",
        "While these `x` and `lengths` tensors are randomly generated, they will \"look\" like real inputs.\n",
        "\n",
        "For clarity:\n",
        "- `x`'s entries range between 0 and the vocab size, which is the valid range of input token IDs for the model.\n",
        "- `x` has shape `(batch_size, max_length)`, simulating a batch of `batch_size` sequences padded/truncated to length `max_length`.\n",
        "- `x` has datatype `torch.int64`, which is the kind of tensor our tokenizer will produce.\n",
        "- `x` is on the GPU if it's available.\n",
        "- `lengths` has entries ranging between 1 and `max_length`, simulating possible \"real\" sequence lengths, ignoring padding.\n",
        "- `lengths` has shape `(batch_size,)`, simulating a 1D array of \"true\" lengths (ignoring padding), one \"true\" length per item in the batch.\n",
        "- `lengths` has datatype `torch.int64`, which is the kind of tensor the RNN utilities expect.\n",
        "- `lengths` is on the CPU, also because it's what the RNN utilities expect."
      ],
      "metadata": {
        "id": "Zo_l8hE3pCjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model's forward() method\n",
        "rnn(\n",
        "    x = torch.randint( # random valid x\n",
        "        0,\n",
        "        tokenizer.vocab_size,\n",
        "        (batch_size, max_length),\n",
        "        dtype=torch.int64,\n",
        "        device=device\n",
        "    ),\n",
        "    lengths = torch.randint( # random valid lengths\n",
        "        1,\n",
        "        max_length,\n",
        "        (batch_size,),\n",
        "        dtype=torch.int64,\n",
        "        device='cpu'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "5oLFZGuUCp64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders"
      ],
      "metadata": {
        "id": "hRP0E5aXmgo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is a convenient wrapper for a dataset. You can iterate over it to access batches of data.\n",
        "- `dataset` is the dataset you want to wrap.\n",
        "- `batch_size` is the batch size.\n",
        "- `shuffle=True` will randomly shuffle the data each time you iterate over the `DataLoader`.\n",
        "- `num_workers` will spawn additional processes on CPU to load up batches more efficiently."
      ],
      "metadata": {
        "id": "8dpXoiYerP0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A brief note on data size\n",
        "\n",
        "You may find that it is very difficult to get the model to learn the entire training set, as it's very large. Consider adjusting `NUM_SAMPLES` larger and larger as you experiment. Try to get the model to learn a few tens of thousands of training examples, and evaluate on the same amount of testing data."
      ],
      "metadata": {
        "id": "aXYis5Typuo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OVERFIT = False # test on the train set to check whether the model is learning\n",
        "NUM_SAMPLES = 10_000 # number of train samples, also number of test samples\n",
        "PER_CLASS = int(NUM_SAMPLES / 2)\n",
        "\n",
        "train_set = DataLoader(\n",
        "    dataset=IMDBDataset('train')[:PER_CLASS] + IMDBDataset('train')[-PER_CLASS:],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=1\n",
        ")\n",
        "\n",
        "if OVERFIT: # test on the train set to check whether the model is learning\n",
        "    test_set = train_set\n",
        "else:\n",
        "    test_set = DataLoader(\n",
        "        dataset=IMDBDataset('test')[:PER_CLASS] + IMDBDataset('test')[-PER_CLASS:],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=1\n",
        "    )"
      ],
      "metadata": {
        "id": "zVlWAIUK24KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop with evaluation at end of each epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "shN6SXP7sVoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TODO: Complete the training [20 points] and evaluation [10 points] loop below. Comments have been left to help you.***\n",
        "\n",
        "Note: computing the lengths is tricky. The idea is to count how many tokens in each sequence are NOT padding tokens. For instance, if 0 is our padding index, we might have:\n",
        "\n",
        "`inputs = [ [1, 2, 0, 0], [5, 8, 3, 0] ]`\n",
        "\n",
        "Then `lengths = [2, 3]`."
      ],
      "metadata": {
        "id": "8rGP5yXXtdQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# track loss\n",
        "train_losses = []\n",
        "\n",
        "# track time\n",
        "times = []\n",
        "\n",
        "# after log_freq batches, print loss and time\n",
        "log_freq = 20\n",
        "\n",
        "for e in range(epochs): # epoch loop\n",
        "\n",
        "    print(f'Epoch {e+1}:')\n",
        "\n",
        "    # iterate over training data in batches\n",
        "    for idx, batch in enumerate(train_set):\n",
        "\n",
        "        start = time()\n",
        "\n",
        "        # TODO: unpack batch\n",
        "\n",
        "\n",
        "        # TODO: tokenize entire batch\n",
        "\n",
        "\n",
        "        # TODO: determine \"true\" lengths, ignoring padding\n",
        "\n",
        "\n",
        "        #print(tokens.shape, labels.shape)\n",
        "\n",
        "        # TODO: reset any gradients from previous batch to 0\n",
        "\n",
        "\n",
        "        # TODO: pass through RNN - ensure lengths is on CPU\n",
        "\n",
        "        # print(outputs.shape, labels.shape)\n",
        "        # print(outputs.dtype, labels.dtype)\n",
        "\n",
        "        # TODO: compute the loss between the outputs and the labels\n",
        "        # labels must be floats and on the same device as the outputs\n",
        "\n",
        "\n",
        "        # TODO: backpropagation\n",
        "\n",
        "\n",
        "        # TODO: update model parameters\n",
        "\n",
        "\n",
        "        # track loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # track time\n",
        "        times.append(time() - start)\n",
        "\n",
        "        # track average loss and time\n",
        "        avg_loss = sum(train_losses) / len(train_losses)\n",
        "        avg_time = sum(times) / len(times)\n",
        "\n",
        "        # log message\n",
        "        if (idx+1) % log_freq == 0:\n",
        "            msg =  f'\\tBatch: {idx+1:04}, '\n",
        "            msg += f'Loss: {train_losses[-1]:.4f}, Avg Loss: {avg_loss:.4f}, '\n",
        "            msg += f'Time: {times[-1]:.4f}, Avg Time: {avg_time:.4f}'\n",
        "            print(msg)\n",
        "\n",
        "    # predictions over test set\n",
        "    test_preds = []\n",
        "\n",
        "    # true labels over test set\n",
        "    test_labels = []\n",
        "\n",
        "    # turn off gradient computation to save time - we are not updating here\n",
        "    with torch.no_grad():\n",
        "\n",
        "        print(f'Evaluating after epoch {e+1}:')\n",
        "\n",
        "        # iterate over test set\n",
        "        for idx, batch in enumerate(test_set):\n",
        "\n",
        "            # TODO: unpack batch\n",
        "\n",
        "\n",
        "            # TODO: tokenize entire batch\n",
        "\n",
        "\n",
        "            # TODO: determine \"true\" lengths, ignoring padding\n",
        "\n",
        "\n",
        "            # TODO: pass through RNN - ensure lengths is on CPU\n",
        "\n",
        "\n",
        "            # make predictions based on model outputs\n",
        "            for output in outputs:\n",
        "                if output > 0: # BCE trained this output to be positive\n",
        "                    test_preds.append(1) # associated to positive label\n",
        "                else: # BCE trained this output to be negative\n",
        "                    test_preds.append(0) # associated to negative label\n",
        "\n",
        "            # store the corresponding labels\n",
        "            for label in labels:\n",
        "                test_labels.append(label)\n",
        "\n",
        "    # evaluation\n",
        "    print(classification_report(test_labels, test_preds))"
      ],
      "metadata": {
        "id": "-JfYqKmIWv7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
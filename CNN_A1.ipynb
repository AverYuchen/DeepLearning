{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AverYuchen/DeepLearning/blob/main/CNN_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNNs in PyTorch\n",
        "\n",
        "In this assignment, you'll implement some Convolutional Neural Networks (CNNs) in PyTorch."
      ],
      "metadata": {
        "id": "AV9LBf4nBDg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up\n",
        "\n",
        "We'll start by importing the following:\n",
        "- [`torch`](https://pytorch.org/docs/stable/torch.html) - the core PyTorch library.\n",
        "- [`torch.nn`](https://pytorch.org/docs/stable/nn.html) - a module containing building blocks for NNs such as linear layers, convolutional layers, and so on.\n",
        "- [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html) - a module containing activation functions, loss functions, and so on.\n",
        "- [`torch.optim`](https://pytorch.org/docs/stable/optim.html) - a module containing optimizers which update the parameters of a NN.\n",
        "- [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) in [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) - Can be used to batch data together and iterate over batches, shuffle data, and parallelize the training process to speed it up.\n",
        "- [`MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html) in [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) - The [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a collection of images of handwritten digits.\n",
        "- [`ToTensor`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor) in [`torchvision.transforms`](https://pytorch.org/vision/0.9/transforms.html) - Converts PIL images or NumPy arrays to PyTorch tensors."
      ],
      "metadata": {
        "id": "ppx9vYd7U9fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR10, MNIST\n",
        "from torchvision.transforms.v2 import ToTensor\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "l74HSkVDtdzo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Let's define a transformation for the [CIFAR10 dataset](https://en.wikipedia.org/wiki/CIFAR-10).\n",
        "\n",
        "We'll first cast the images to PyTorch tensors using [`transforms.ToTensor()`](https://pytorch.org/vision/master/generated/torchvision.transforms.ToTensor.html). These tensors are automatically normalized such that their values are between 0 and 1.\n",
        "\n",
        "Then, we'll re-normalize the pixel values with [`transforms.Normalize()`](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html) to conform approximately to a standard normal distribution, assuming the mean and standard deviation of any channel of the returned tensor to be 0.5. This is not an unreasonable assumption. It's also a fairly standard thing to do to squash inputs to be in (or close to) the range [-1,1], which is where neural networks work best in terms of converging when performing optimization."
      ],
      "metadata": {
        "id": "pmcSEpkOcA6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 transform - three channels, normalize with 3 means and 3 SDs\n",
        "cifar_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# MNIST transform - single channel, so only 1 mean and 1 SD\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1308,), (0.3016,))\n",
        "])"
      ],
      "metadata": {
        "id": "OjvbG0f7hF3N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mnist = MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=mnist_transform\n",
        ")\n",
        "test_mnist = MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=mnist_transform\n",
        ")"
      ],
      "metadata": {
        "id": "eZDYwqGcv0J3",
        "outputId": "00b38607-6b7b-47bc-91eb-b701cb6b4d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 14842706.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 410447.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3637081.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10455833.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load up the CIFAR-10 dataset. You can specify the split you want using `train=True|False`. `root` is the directory where the dataset will be saved. You can also directly apply the transform from the previous cell by specifying `transform`."
      ],
      "metadata": {
        "id": "8RI612Kn8jUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "# CIFAR10 data\n",
        "train_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "val_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "'''\n",
        "test_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "\n",
        "val_size = 5000\n",
        "test_size = len(test_dataset) - val_size\n",
        "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
        "'''"
      ],
      "metadata": {
        "id": "Ob7JPEEabfTR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "5470a8b9-acec-41ca-9cd6-df1042b74e8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43026663.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntest_dataset = CIFAR10(\\n    root='./data',\\n    train=False,\\n    download=True,\\n    transform=cifar_transform\\n)\\n\\n\\nval_size = 5000\\ntest_size = len(test_dataset) - val_size\\nval_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define `DataLoader` objects for the CIFAR10 data now.\n",
        "\n",
        "We'll use a (mini) batch size of 32. It's common to use powers of 2 in deep learning because it's more efficient to handle such numbers on hardware.\n",
        "\n",
        "We'll define separate `DataLoader` objects to handle our training and test splits to avoid data leakage (training on the test set or testing on the train set).\n",
        "\n",
        "We'll also have the `DataLoader` objects shuffle our data whenever we iterate over them (`shuffle=True`). Shuffling data at each epoch is beneficial in that the model won't be optimized in a way that depends on a specific ordering of the data.\n",
        "\n",
        "Finally, we'll parallelize the loading of the data using 4 CPU processes to load data (`num_workers=4`)."
      ],
      "metadata": {
        "id": "lb2XIf19cnsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders for MNIST\n",
        "batch_size = 32\n",
        "\n",
        "train_loader_mnist = DataLoader(\n",
        "    train_mnist,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader_mnist = DataLoader(\n",
        "    test_mnist,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "VrVIt1Lhz2RX",
        "outputId": "24a9a2dc-6906-469f-e5a0-60faf4bd172c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "'''\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "93SCDUnGji8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "78540425-aa82-481c-ff42-445d889b210a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_loader = DataLoader(\\n    test_dataset,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=4\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and training CNNs\n",
        "\n",
        "We'll define `criterion` to be [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), a common loss function used to train classification models.\n",
        "\n",
        "We'll also define a Stochastic Gradient Descent optmizizer ([`optim.SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)), which will optimize the parameters of `net`. We'll set two hyperparameters manually: the learning rate (`lr=0.001`) and the momentum (`momentum=0.9`)."
      ],
      "metadata": {
        "id": "KZpzVZUjcJSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss fuction and optimizer\n",
        "def get_crit_and_opt(net):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    return criterion, optimizer"
      ],
      "metadata": {
        "id": "yNEwqv8rkQNM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how [LeNet5](https://ieeexplore.ieee.org/document/726791) (Lecun et al. 1998) is implemented. The architecture looks something like this:"
      ],
      "metadata": {
        "id": "f1kVjROgsGyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1PwYfmSXqBnosIQi-ewrr03Ibd_lmRtea)"
      ],
      "metadata": {
        "id": "R-mgpop1bcN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet5 is compatible with the MNIST dataset. Let's see how to implement the architecture in PyTorch:"
      ],
      "metadata": {
        "id": "aAbXzjDJsrWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # 6 input channels to 16 output channels with 5x5 convolution\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # affine operations: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 16 channels each of size 5x5 to 1x120 vector\n",
        "        self.fc2 = nn.Linear(120, 84) # 1x120 vector to 1x84 vector\n",
        "        self.fc3 = nn.Linear(84, 10) # 1x84 vector to 1x10 vector\n",
        "\n",
        "    def forward(self, x):\n",
        "        # average pooling over a 2x2 window\n",
        "        x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.avg_pool2d(F.sigmoid(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.sigmoid(self.fc1(x)) # linear + sig activation\n",
        "        x = F.sigmoid(self.fc2(x)) # linear + sig activation\n",
        "        x = self.fc3(x) # linear\n",
        "        return x"
      ],
      "metadata": {
        "id": "3wg7aZ5PFfem"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, a PyTorch neural network definition must:\n",
        "- subclass [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
        "- call `super().__init__()` in the constructor (`__init__()`) method\n",
        "- define the trainable parameters/layers (convolutions, linears, poolings, etc.) in the constructor\n",
        "- define what should happen to the inputs in the `forward()` method"
      ],
      "metadata": {
        "id": "9BxZoUpss4Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your own model for the MNIST data here [20 pts]:\n",
        "class CNN_Network1(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN_Network1, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "    self.conv2 = nn.Conv2d(8, 22, 5)\n",
        "    self.fc1 = nn.Linear(22*4*4, 125)\n",
        "    self.fc2 = nn.Linear(125, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "  def forward(self, x):\n",
        "    x = F.avg_pool2d(F.sigmoid(self.conv1(x)), (2, 2))\n",
        "    # If the size is a square, you can specify with a single number\n",
        "    x = F.avg_pool2d(F.sigmoid(self.conv2(x)), 2)\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "    x = F.sigmoid(self.fc1(x)) # linear + sig activation\n",
        "    x = F.sigmoid(self.fc2(x)) # linear + sig activation\n",
        "    x = self.fc3(x) # linear + sig activation\n",
        "    return x"
      ],
      "metadata": {
        "id": "Wdp9RkPobRuC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your own model for the CIFAR10 data here [20 pts]:\n",
        "class CNN_Network2(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN_Network2, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 36, 5)\n",
        "    self.fc1 = nn.Linear(36 * 5 * 5, 180)\n",
        "    self.fc2 = nn.Linear(180, 120)\n",
        "    self.fc3 = nn.Linear(120, 84)\n",
        "    self.fc4 = nn.Linear(84, 10)\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 36 * 5 * 5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Z8GCyZDZ6iPv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a useful object for tracking losses/performance during training and dev."
      ],
      "metadata": {
        "id": "pk8hMo_vxzhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "\n",
        "    \"\"\"Computes and stores an average and current value.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "ar5E7QTSw56k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define an accuracy metric that flexibly computes top-k accuracies."
      ],
      "metadata": {
        "id": "Cq2ZTU4-19Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_rate(output, target, topk=(1,)):\n",
        "\n",
        "    \"\"\"Computes the top-k error rate for the specified values of k.\"\"\"\n",
        "\n",
        "    maxk = max(topk) # largest k we'll need to work with\n",
        "    batch_size = target.size(0) # determine batch size\n",
        "\n",
        "    # get maxk best predictions for each item in the batch, both values and indices\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "\n",
        "    # reshape predictions and targets and compare them element-wise\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk: # for each top-k accuracy we want\n",
        "\n",
        "        # num correct\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        # num incorrect\n",
        "        wrong_k = batch_size - correct_k\n",
        "        # as a percentage\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "er1n5plIlT86"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you connect to a runtime with a T4 available, this line will ensure computations that can be done on the T4 are done there."
      ],
      "metadata": {
        "id": "p15LFjPHx6Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Xaru9TTXbAAG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training function below takes the training set's `DataLoader`, the model we are training, the loss function we are using, and the optimizer for this model.\n",
        "\n",
        "It then trains the model on the data for 1 epoch."
      ],
      "metadata": {
        "id": "-8WU1ms3yszg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training function - 1 epoch\n",
        "def train(\n",
        "    train_loader,\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    epoch,\n",
        "    epochs,\n",
        "    print_freq = 100,\n",
        "    verbose = True\n",
        "):\n",
        "\n",
        "    # track average and worst losses\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # set training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over data - automatically shuffled\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        # put batch of image tensors on GPU\n",
        "        images = images.to(device)\n",
        "        # put batch of label tensors on GPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # model output\n",
        "        outputs = model(images)\n",
        "\n",
        "        # loss computation\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # update meter with the value of the loss once for each item in the batch\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "\n",
        "        # logging during epoch\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print(\n",
        "                f'Epoch: [{epoch+1}/{epochs}][{i:4}/{len(train_loader)}]\\t'\n",
        "                f'Loss: {losses.val:.4f} ({losses.avg:.4f} on avg)'\n",
        "            )\n",
        "\n",
        "    # log again at end of epoch\n",
        "    print(f'\\n* Epoch: [{epoch+1}/{epochs}]\\tTrain loss: {losses.avg:.3f}\\n')\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "id": "kFMDxaNDlRw0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val function\n",
        "def validate(\n",
        "    val_loader,\n",
        "    model,\n",
        "    criterion,\n",
        "    epoch,\n",
        "    epochs,\n",
        "    print_freq = 100,\n",
        "    verbose = True\n",
        "):\n",
        "\n",
        "    # track average and worst losses and batch-wise top-1 and top-5 accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # set evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # iterate over data - automatically shuffled\n",
        "    for i, (images, labels) in enumerate(val_loader):\n",
        "\n",
        "        # put batch of image tensors on GPU\n",
        "        images = images.to(device)\n",
        "        # put batch of label tensors on GPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # model output\n",
        "        output = model(images)\n",
        "\n",
        "        # loss computation\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # top-1 and top-5 accuracy on this batch\n",
        "        err1, err5, = error_rate(output.data, labels, topk=(1, 5))\n",
        "\n",
        "        # update meters with the value of the loss once for each item in the batch\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        # update meters with top-1 and top-5 accuracy on this batch once for each item in the batch\n",
        "        top1.update(err1.item(), images.size(0))\n",
        "        top5.update(err5.item(), images.size(0))\n",
        "\n",
        "        # logging during epoch\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print(\n",
        "                f'Test (on val set): [{epoch+1}/{epochs}][{i:4}/{len(val_loader)}]\\t'\n",
        "                f'Loss: {losses.val:.4f} ({losses.avg:.4f} on avg)\\t'\n",
        "                f'Top-1 err: {top1.val:.4f} ({top1.avg:.4f} on avg)\\t'\n",
        "                f'Top-5 err: {top5.val:.4f} ({top5.avg:.4f} on avg)'\n",
        "            )\n",
        "\n",
        "    # logging for end of epoch\n",
        "    print(\n",
        "        f'\\n* Epoch: [{epoch+1}/{epochs}]\\t'\n",
        "        f'Test loss: {losses.avg:.3f}\\t'\n",
        "        f'Top-1 err: {top1.avg:.3f}\\t'\n",
        "        f'Top-5 err: {top5.avg:.3f}\\n'\n",
        "    )\n",
        "\n",
        "    # avergae top-1 and top-5 accuracies batch-wise, and average loss batch-wise\n",
        "    return output, labels, top1.avg, top5.avg, losses.avg"
      ],
      "metadata": {
        "id": "DDJnoe7Pu5ez"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best error rates so far\n",
        "best_err1 = 100\n",
        "best_err5 = 100"
      ],
      "metadata": {
        "id": "AKwJjLLU11vg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test MNIST Model\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # select a model to train here\n",
        "    model_mnist = CNN_Network1()\n",
        "\n",
        "    # move to GPU\n",
        "    model_mnist.to(device)\n",
        "\n",
        "    # select number of epochs\n",
        "    epochs = 3\n",
        "\n",
        "    # get criterion and optimizer\n",
        "    criterion, optimizer = get_crit_and_opt(model_mnist)\n",
        "\n",
        "    # epoch loop\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        train_loss_mnist = train(\n",
        "          train_loader_mnist,\n",
        "          model_mnist,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # evaluate on validation set\n",
        "        predictions_mnist, labels_mnist, err1_mnist, err5_mnist, val_loss_mnist = validate(\n",
        "          val_loader_mnist,\n",
        "          model_mnist,\n",
        "          criterion,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = err1_mnist <= best_err1\n",
        "        best_err1 = min(err1_mnist, best_err1)\n",
        "        if is_best:\n",
        "            best_err5 = err5_mnist\n",
        "\n",
        "        print('Current best error rate (top-1 and top-5 error):', best_err1, best_err5, '\\n')\n",
        "    print('Best error rate (top-1 and top-5 error):', best_err1, best_err5)"
      ],
      "metadata": {
        "id": "p2ES6XLivcTq",
        "outputId": "e4502784-5e83-4cd3-a34c-686b851bffb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/3][   0/1875]\tLoss: 2.3672 (2.3672 on avg)\n",
            "Epoch: [1/3][ 100/1875]\tLoss: 2.2850 (2.3163 on avg)\n",
            "Epoch: [1/3][ 200/1875]\tLoss: 2.2948 (2.3090 on avg)\n",
            "Epoch: [1/3][ 300/1875]\tLoss: 2.3060 (2.3075 on avg)\n",
            "Epoch: [1/3][ 400/1875]\tLoss: 2.3058 (2.3062 on avg)\n",
            "Epoch: [1/3][ 500/1875]\tLoss: 2.3056 (2.3059 on avg)\n",
            "Epoch: [1/3][ 600/1875]\tLoss: 2.2979 (2.3055 on avg)\n",
            "Epoch: [1/3][ 700/1875]\tLoss: 2.2924 (2.3051 on avg)\n",
            "Epoch: [1/3][ 800/1875]\tLoss: 2.3056 (2.3048 on avg)\n",
            "Epoch: [1/3][ 900/1875]\tLoss: 2.3091 (2.3046 on avg)\n",
            "Epoch: [1/3][1000/1875]\tLoss: 2.3007 (2.3045 on avg)\n",
            "Epoch: [1/3][1100/1875]\tLoss: 2.3133 (2.3042 on avg)\n",
            "Epoch: [1/3][1200/1875]\tLoss: 2.2990 (2.3041 on avg)\n",
            "Epoch: [1/3][1300/1875]\tLoss: 2.3024 (2.3039 on avg)\n",
            "Epoch: [1/3][1400/1875]\tLoss: 2.3077 (2.3039 on avg)\n",
            "Epoch: [1/3][1500/1875]\tLoss: 2.3036 (2.3038 on avg)\n",
            "Epoch: [1/3][1600/1875]\tLoss: 2.2883 (2.3036 on avg)\n",
            "Epoch: [1/3][1700/1875]\tLoss: 2.2806 (2.3035 on avg)\n",
            "Epoch: [1/3][1800/1875]\tLoss: 2.2942 (2.3035 on avg)\n",
            "\n",
            "* Epoch: [1/3]\tTrain loss: 2.303\n",
            "\n",
            "Test (on val set): [1/3][   0/313]\tLoss: 2.3086 (2.3086 on avg)\tTop-1 err: 87.5000 (87.5000 on avg)\tTop-5 err: 50.0000 (50.0000 on avg)\n",
            "Test (on val set): [1/3][ 100/313]\tLoss: 2.3057 (2.3016 on avg)\tTop-1 err: 90.6250 (89.3564 on avg)\tTop-5 err: 50.0000 (48.1126 on avg)\n",
            "Test (on val set): [1/3][ 200/313]\tLoss: 2.2969 (2.3012 on avg)\tTop-1 err: 90.6250 (89.4745 on avg)\tTop-5 err: 40.6250 (47.9011 on avg)\n",
            "Test (on val set): [1/3][ 300/313]\tLoss: 2.3145 (2.3017 on avg)\tTop-1 err: 90.6250 (89.7529 on avg)\tTop-5 err: 62.5000 (48.4635 on avg)\n",
            "\n",
            "* Epoch: [1/3]\tTest loss: 2.302\tTop-1 err: 89.720\tTop-5 err: 48.490\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 58.52 9.17 \n",
            "\n",
            "Epoch: [2/3][   0/1875]\tLoss: 2.2983 (2.2983 on avg)\n",
            "Epoch: [2/3][ 100/1875]\tLoss: 2.2819 (2.3007 on avg)\n",
            "Epoch: [2/3][ 200/1875]\tLoss: 2.2737 (2.3021 on avg)\n",
            "Epoch: [2/3][ 300/1875]\tLoss: 2.3005 (2.3017 on avg)\n",
            "Epoch: [2/3][ 400/1875]\tLoss: 2.2969 (2.3025 on avg)\n",
            "Epoch: [2/3][ 500/1875]\tLoss: 2.2903 (2.3024 on avg)\n",
            "Epoch: [2/3][ 600/1875]\tLoss: 2.2904 (2.3022 on avg)\n",
            "Epoch: [2/3][ 700/1875]\tLoss: 2.2983 (2.3023 on avg)\n",
            "Epoch: [2/3][ 800/1875]\tLoss: 2.3099 (2.3024 on avg)\n",
            "Epoch: [2/3][ 900/1875]\tLoss: 2.3050 (2.3024 on avg)\n",
            "Epoch: [2/3][1000/1875]\tLoss: 2.3240 (2.3026 on avg)\n",
            "Epoch: [2/3][1100/1875]\tLoss: 2.2834 (2.3026 on avg)\n",
            "Epoch: [2/3][1200/1875]\tLoss: 2.3207 (2.3026 on avg)\n",
            "Epoch: [2/3][1300/1875]\tLoss: 2.3155 (2.3027 on avg)\n",
            "Epoch: [2/3][1400/1875]\tLoss: 2.3083 (2.3026 on avg)\n",
            "Epoch: [2/3][1500/1875]\tLoss: 2.2973 (2.3027 on avg)\n",
            "Epoch: [2/3][1600/1875]\tLoss: 2.3128 (2.3027 on avg)\n",
            "Epoch: [2/3][1700/1875]\tLoss: 2.2941 (2.3027 on avg)\n",
            "Epoch: [2/3][1800/1875]\tLoss: 2.3062 (2.3028 on avg)\n",
            "\n",
            "* Epoch: [2/3]\tTrain loss: 2.303\n",
            "\n",
            "Test (on val set): [2/3][   0/313]\tLoss: 2.3048 (2.3048 on avg)\tTop-1 err: 93.7500 (93.7500 on avg)\tTop-5 err: 46.8750 (46.8750 on avg)\n",
            "Test (on val set): [2/3][ 100/313]\tLoss: 2.3078 (2.3016 on avg)\tTop-1 err: 90.6250 (88.5829 on avg)\tTop-5 err: 46.8750 (48.1745 on avg)\n",
            "Test (on val set): [2/3][ 200/313]\tLoss: 2.2899 (2.3017 on avg)\tTop-1 err: 71.8750 (88.3862 on avg)\tTop-5 err: 40.6250 (48.5386 on avg)\n",
            "Test (on val set): [2/3][ 300/313]\tLoss: 2.3099 (2.3020 on avg)\tTop-1 err: 93.7500 (88.6005 on avg)\tTop-5 err: 56.2500 (48.6503 on avg)\n",
            "\n",
            "* Epoch: [2/3]\tTest loss: 2.302\tTop-1 err: 88.650\tTop-5 err: 48.730\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 58.52 9.17 \n",
            "\n",
            "Epoch: [3/3][   0/1875]\tLoss: 2.2996 (2.2996 on avg)\n",
            "Epoch: [3/3][ 100/1875]\tLoss: 2.3042 (2.2999 on avg)\n",
            "Epoch: [3/3][ 200/1875]\tLoss: 2.2882 (2.2998 on avg)\n",
            "Epoch: [3/3][ 300/1875]\tLoss: 2.2838 (2.3009 on avg)\n",
            "Epoch: [3/3][ 400/1875]\tLoss: 2.3117 (2.3016 on avg)\n",
            "Epoch: [3/3][ 500/1875]\tLoss: 2.2811 (2.3014 on avg)\n",
            "Epoch: [3/3][ 600/1875]\tLoss: 2.3083 (2.3017 on avg)\n",
            "Epoch: [3/3][ 700/1875]\tLoss: 2.3130 (2.3019 on avg)\n",
            "Epoch: [3/3][ 800/1875]\tLoss: 2.3254 (2.3021 on avg)\n",
            "Epoch: [3/3][ 900/1875]\tLoss: 2.3051 (2.3023 on avg)\n",
            "Epoch: [3/3][1000/1875]\tLoss: 2.2963 (2.3024 on avg)\n",
            "Epoch: [3/3][1100/1875]\tLoss: 2.3130 (2.3025 on avg)\n",
            "Epoch: [3/3][1200/1875]\tLoss: 2.2915 (2.3025 on avg)\n",
            "Epoch: [3/3][1300/1875]\tLoss: 2.3293 (2.3026 on avg)\n",
            "Epoch: [3/3][1400/1875]\tLoss: 2.2908 (2.3026 on avg)\n",
            "Epoch: [3/3][1500/1875]\tLoss: 2.3093 (2.3027 on avg)\n",
            "Epoch: [3/3][1600/1875]\tLoss: 2.2877 (2.3027 on avg)\n",
            "Epoch: [3/3][1700/1875]\tLoss: 2.3050 (2.3027 on avg)\n",
            "Epoch: [3/3][1800/1875]\tLoss: 2.2750 (2.3027 on avg)\n",
            "\n",
            "* Epoch: [3/3]\tTrain loss: 2.303\n",
            "\n",
            "Test (on val set): [3/3][   0/313]\tLoss: 2.2927 (2.2927 on avg)\tTop-1 err: 90.6250 (90.6250 on avg)\tTop-5 err: 40.6250 (40.6250 on avg)\n",
            "Test (on val set): [3/3][ 100/313]\tLoss: 2.3063 (2.3020 on avg)\tTop-1 err: 93.7500 (87.8403 on avg)\tTop-5 err: 53.1250 (49.1646 on avg)\n",
            "Test (on val set): [3/3][ 200/313]\tLoss: 2.2925 (2.3022 on avg)\tTop-1 err: 84.3750 (88.2307 on avg)\tTop-5 err: 43.7500 (48.9894 on avg)\n",
            "Test (on val set): [3/3][ 300/313]\tLoss: 2.2921 (2.3023 on avg)\tTop-1 err: 90.6250 (88.6939 on avg)\tTop-5 err: 40.6250 (48.8787 on avg)\n",
            "\n",
            "* Epoch: [3/3]\tTest loss: 2.302\tTop-1 err: 88.650\tTop-5 err: 48.950\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 58.52 9.17 \n",
            "\n",
            "Best error rate (top-1 and top-5 error): 58.52 9.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function.\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # select a model to train here\n",
        "    model = CNN_Network2()\n",
        "\n",
        "    # move to GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # select number of epochs\n",
        "    epochs = 3\n",
        "\n",
        "    # get criterion and optimizer\n",
        "    criterion, optimizer = get_crit_and_opt(model)\n",
        "\n",
        "    # epoch loop\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        train_loss = train(\n",
        "          train_loader,\n",
        "          model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # evaluate on validation set\n",
        "        predictions, labels, err1, err5, val_loss = validate(\n",
        "          val_loader,\n",
        "          model,\n",
        "          criterion,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = err1 <= best_err1\n",
        "        best_err1 = min(err1, best_err1)\n",
        "        if is_best:\n",
        "            best_err5 = err5\n",
        "\n",
        "        print('Current best error rate (top-1 and top-5 error):', best_err1, best_err5, '\\n')\n",
        "    print('Best error rate (top-1 and top-5 error):', best_err1, best_err5)"
      ],
      "metadata": {
        "id": "hGxrBqkt0IsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be741e9d-ac0e-4e1f-d63e-00d9268c22ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/3][   0/1563]\tLoss: 2.2926 (2.2926 on avg)\n",
            "Epoch: [1/3][ 100/1563]\tLoss: 2.3048 (2.3032 on avg)\n",
            "Epoch: [1/3][ 200/1563]\tLoss: 2.2982 (2.3033 on avg)\n",
            "Epoch: [1/3][ 300/1563]\tLoss: 2.3068 (2.3032 on avg)\n",
            "Epoch: [1/3][ 400/1563]\tLoss: 2.3010 (2.3027 on avg)\n",
            "Epoch: [1/3][ 500/1563]\tLoss: 2.3136 (2.3029 on avg)\n",
            "Epoch: [1/3][ 600/1563]\tLoss: 2.2959 (2.3027 on avg)\n",
            "Epoch: [1/3][ 700/1563]\tLoss: 2.3045 (2.3024 on avg)\n",
            "Epoch: [1/3][ 800/1563]\tLoss: 2.3065 (2.3022 on avg)\n",
            "Epoch: [1/3][ 900/1563]\tLoss: 2.2987 (2.3019 on avg)\n",
            "Epoch: [1/3][1000/1563]\tLoss: 2.2991 (2.3017 on avg)\n",
            "Epoch: [1/3][1100/1563]\tLoss: 2.2935 (2.3014 on avg)\n",
            "Epoch: [1/3][1200/1563]\tLoss: 2.2968 (2.3010 on avg)\n",
            "Epoch: [1/3][1300/1563]\tLoss: 2.2940 (2.3006 on avg)\n",
            "Epoch: [1/3][1400/1563]\tLoss: 2.2990 (2.3001 on avg)\n",
            "Epoch: [1/3][1500/1563]\tLoss: 2.2809 (2.2994 on avg)\n",
            "\n",
            "* Epoch: [1/3]\tTrain loss: 2.299\n",
            "\n",
            "Test (on val set): [1/3][   0/313]\tLoss: 2.2893 (2.2893 on avg)\tTop-1 err: 87.5000 (87.5000 on avg)\tTop-5 err: 43.7500 (43.7500 on avg)\n",
            "Test (on val set): [1/3][ 100/313]\tLoss: 2.2687 (2.2833 on avg)\tTop-1 err: 71.8750 (81.1262 on avg)\tTop-5 err: 15.6250 (35.7983 on avg)\n",
            "Test (on val set): [1/3][ 200/313]\tLoss: 2.2729 (2.2822 on avg)\tTop-1 err: 81.2500 (80.2705 on avg)\tTop-5 err: 31.2500 (35.4633 on avg)\n",
            "Test (on val set): [1/3][ 300/313]\tLoss: 2.2718 (2.2827 on avg)\tTop-1 err: 75.0000 (80.6686 on avg)\tTop-5 err: 28.1250 (35.8700 on avg)\n",
            "\n",
            "* Epoch: [1/3]\tTest loss: 2.283\tTop-1 err: 80.630\tTop-5 err: 35.860\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 80.63 35.86 \n",
            "\n",
            "Epoch: [2/3][   0/1563]\tLoss: 2.2784 (2.2784 on avg)\n",
            "Epoch: [2/3][ 100/1563]\tLoss: 2.2926 (2.2777 on avg)\n",
            "Epoch: [2/3][ 200/1563]\tLoss: 2.2448 (2.2712 on avg)\n",
            "Epoch: [2/3][ 300/1563]\tLoss: 2.2312 (2.2617 on avg)\n",
            "Epoch: [2/3][ 400/1563]\tLoss: 2.0981 (2.2519 on avg)\n",
            "Epoch: [2/3][ 500/1563]\tLoss: 2.1686 (2.2410 on avg)\n",
            "Epoch: [2/3][ 600/1563]\tLoss: 2.1146 (2.2276 on avg)\n",
            "Epoch: [2/3][ 700/1563]\tLoss: 2.1533 (2.2117 on avg)\n",
            "Epoch: [2/3][ 800/1563]\tLoss: 2.2069 (2.1957 on avg)\n",
            "Epoch: [2/3][ 900/1563]\tLoss: 2.1722 (2.1791 on avg)\n",
            "Epoch: [2/3][1000/1563]\tLoss: 1.9484 (2.1602 on avg)\n",
            "Epoch: [2/3][1100/1563]\tLoss: 2.1009 (2.1439 on avg)\n",
            "Epoch: [2/3][1200/1563]\tLoss: 1.9665 (2.1273 on avg)\n",
            "Epoch: [2/3][1300/1563]\tLoss: 2.1741 (2.1122 on avg)\n",
            "Epoch: [2/3][1400/1563]\tLoss: 1.8407 (2.0971 on avg)\n",
            "Epoch: [2/3][1500/1563]\tLoss: 2.0601 (2.0827 on avg)\n",
            "\n",
            "* Epoch: [2/3]\tTrain loss: 2.074\n",
            "\n",
            "Test (on val set): [2/3][   0/313]\tLoss: 1.7917 (1.7917 on avg)\tTop-1 err: 71.8750 (71.8750 on avg)\tTop-5 err: 12.5000 (12.5000 on avg)\n",
            "Test (on val set): [2/3][ 100/313]\tLoss: 1.9115 (1.8205 on avg)\tTop-1 err: 62.5000 (66.9245 on avg)\tTop-5 err: 18.7500 (14.0780 on avg)\n",
            "Test (on val set): [2/3][ 200/313]\tLoss: 1.7807 (1.8316 on avg)\tTop-1 err: 71.8750 (66.8532 on avg)\tTop-5 err: 15.6250 (14.1014 on avg)\n",
            "Test (on val set): [2/3][ 300/313]\tLoss: 1.7337 (1.8353 on avg)\tTop-1 err: 59.3750 (67.2757 on avg)\tTop-5 err: 12.5000 (14.3480 on avg)\n",
            "\n",
            "* Epoch: [2/3]\tTest loss: 1.835\tTop-1 err: 67.260\tTop-5 err: 14.360\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 67.26 14.36 \n",
            "\n",
            "Epoch: [3/3][   0/1563]\tLoss: 1.9392 (1.9392 on avg)\n",
            "Epoch: [3/3][ 100/1563]\tLoss: 1.7468 (1.8136 on avg)\n",
            "Epoch: [3/3][ 200/1563]\tLoss: 1.7621 (1.8141 on avg)\n",
            "Epoch: [3/3][ 300/1563]\tLoss: 1.6514 (1.8016 on avg)\n",
            "Epoch: [3/3][ 400/1563]\tLoss: 1.7895 (1.7920 on avg)\n",
            "Epoch: [3/3][ 500/1563]\tLoss: 1.6318 (1.7755 on avg)\n",
            "Epoch: [3/3][ 600/1563]\tLoss: 1.9503 (1.7671 on avg)\n",
            "Epoch: [3/3][ 700/1563]\tLoss: 1.6394 (1.7551 on avg)\n",
            "Epoch: [3/3][ 800/1563]\tLoss: 1.7097 (1.7446 on avg)\n",
            "Epoch: [3/3][ 900/1563]\tLoss: 1.6377 (1.7340 on avg)\n",
            "Epoch: [3/3][1000/1563]\tLoss: 1.6639 (1.7240 on avg)\n",
            "Epoch: [3/3][1100/1563]\tLoss: 1.4909 (1.7161 on avg)\n",
            "Epoch: [3/3][1200/1563]\tLoss: 1.7931 (1.7076 on avg)\n",
            "Epoch: [3/3][1300/1563]\tLoss: 1.0806 (1.6982 on avg)\n",
            "Epoch: [3/3][1400/1563]\tLoss: 1.7328 (1.6912 on avg)\n",
            "Epoch: [3/3][1500/1563]\tLoss: 1.6394 (1.6838 on avg)\n",
            "\n",
            "* Epoch: [3/3]\tTrain loss: 1.680\n",
            "\n",
            "Test (on val set): [3/3][   0/313]\tLoss: 1.6088 (1.6088 on avg)\tTop-1 err: 62.5000 (62.5000 on avg)\tTop-5 err: 15.6250 (15.6250 on avg)\n",
            "Test (on val set): [3/3][ 100/313]\tLoss: 1.3648 (1.5894 on avg)\tTop-1 err: 50.0000 (59.1584 on avg)\tTop-5 err: 3.1250 (9.3131 on avg)\n",
            "Test (on val set): [3/3][ 200/313]\tLoss: 1.3526 (1.5809 on avg)\tTop-1 err: 46.8750 (58.7531 on avg)\tTop-5 err: 3.1250 (9.3750 on avg)\n",
            "Test (on val set): [3/3][ 300/313]\tLoss: 1.6326 (1.5673 on avg)\tTop-1 err: 65.6250 (58.3991 on avg)\tTop-5 err: 12.5000 (9.0635 on avg)\n",
            "\n",
            "* Epoch: [3/3]\tTest loss: 1.570\tTop-1 err: 58.520\tTop-5 err: 9.170\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 58.52 9.17 \n",
            "\n",
            "Best error rate (top-1 and top-5 error): 58.52 9.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a SciKit-Learn [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) for one of your models.\n",
        "\n",
        "This report will show accuracy, classwise precision, recall, and F1, as well as averaged metrics over the classes."
      ],
      "metadata": {
        "id": "tl1xnaY-m1kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a classification report for one model [10 pts]\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# get the true classes and model predictions for the test set for one model\n",
        "y_true = []\n",
        "y_pred = []\n",
        "target_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "_, predicted = torch.max(predictions, 1)  # Get the class with the highest score\n",
        "\n",
        "# Move predictions and labels to CPU, then convert to NumPy arrays\n",
        "y_true.extend(labels.cpu().numpy())\n",
        "y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "#target_names = string names of the classes\n",
        "classification_report(y_true, y_pred, target_names=target_names, zero_division=1)\n",
        "print(classification_report(y_true, y_pred, target_names=target_names, zero_division=1))"
      ],
      "metadata": {
        "id": "j6l7WQ7QnDGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55344bcf-2d58-4fee-97fc-c454898ea1e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       plane       1.00      0.00      0.00         1\n",
            "         car       1.00      0.50      0.67         2\n",
            "        bird       1.00      0.00      0.00         3\n",
            "         cat       1.00      0.33      0.50         3\n",
            "        deer       1.00      0.00      0.00         1\n",
            "         dog       1.00      0.00      0.00         1\n",
            "        frog       0.00      0.00      0.00         1\n",
            "       horse       0.33      0.50      0.40         2\n",
            "        ship       1.00      1.00      1.00         1\n",
            "       truck       0.25      1.00      0.40         1\n",
            "\n",
            "    accuracy                           0.31        16\n",
            "   macro avg       0.76      0.33      0.30        16\n",
            "weighted avg       0.81      0.31      0.31        16\n",
            "\n"
          ]
        }
      ]
    }
  ]
}